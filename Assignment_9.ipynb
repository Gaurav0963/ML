{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470b4aee",
   "metadata": {},
   "source": [
    "# Assignment 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9707f2",
   "metadata": {},
   "source": [
    "### Q1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth.\n",
    "\n",
    "ANSWER : Feature engineering is the process of transforming raw data into features that can be used as input to machine learning algorithms. It involves selecting, creating, and transforming variables to improve the performance of the model. Feature engineering is a crucial step in machine learning, as the quality of the features used can significantly impact the accuracy and generalization ability of the model.\n",
    "\n",
    "The following are the various aspects of feature engineering:\n",
    "\n",
    "1. `Feature selection` : It involves selecting the most relevant features for the problem at hand. It helps to reduce the dimensionality of the data, remove irrelevant or redundant features, and improve the model's performance. Feature selection can be performed using various methods, including statistical tests, correlation analysis, and regularization techniques.\n",
    "\n",
    "2. `Feature extraction` : It involves creating new features from the existing ones. It can help to capture complex patterns in the data and improve the model's performance. Feature extraction techniques include principal component analysis (PCA), independent component analysis (ICA), and singular value decomposition (SVD).\n",
    "\n",
    "3. `Feature scaling` : It involves scaling the features to a common scale to avoid bias towards certain features. It can help to improve the model's performance and reduce overfitting. Common feature scaling techniques include standardization and normalization.\n",
    "\n",
    "4. `Feature transformation` : It involves transforming the features into a different space to make them more suitable for the model. It can help to capture non-linear relationships in the data and improve the model's performance. Common feature transformation techniques include polynomial transformation, logarithmic transformation, and box-cox transformation.\n",
    "\n",
    "5. `Feature engineering for time-series data` : Time-series data has its unique characteristics, and feature engineering for time-series data involves creating features that capture trends, seasonality, and cyclic patterns. Common techniques used for feature engineering time-series data include lagging, differencing, rolling windows, and Fourier transforms.\n",
    "\n",
    "In summary, feature engineering is an essential step in machine learning, and it involves selecting, creating, and transforming features to improve the model's performance. The quality of the features used can significantly impact the accuracy and generalization ability of the model, and various techniques can be used to perform feature selection, feature extraction, feature scaling, and feature transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47012585",
   "metadata": {},
   "source": [
    "### Q2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?\n",
    "\n",
    "ANSWER : Feature selection is the process of selecting a subset of the most important features from the original set of features to be used as input for machine learning models. The goal of feature selection is to reduce the dimensionality of the data, remove irrelevant or redundant features, and improve the model's accuracy and generalization ability.\n",
    "\n",
    "The various methods of feature selection include:\n",
    "\n",
    "1. `Filter Methods` : Evaluates the relevance of each feature independently of the target variable. They use statistical tests, correlation analysis, or other measures to rank the features based on their importance. Common filter methods include chi-square test, mutual information, correlation coefficient, and variance threshold.\n",
    "\n",
    "2. `Wrapper Methods` : It evaluates the performance of the model on a subset of features selected iteratively. They use a search algorithm, such as forward selection, backward elimination, or recursive feature elimination, to select the best subset of features that maximize the model's performance. Wrapper methods can be computationally expensive, but they typically lead to better performance than filter methods.\n",
    "\n",
    "3. `Embedded Methods` : It combine feature selection with the training of the model. They select the most relevant features during the model training process, using regularization techniques or decision tree-based algorithms. Embedded methods are efficient and can improve the model's performance, but they may not be suitable for all types of models.\n",
    "\n",
    "The choice of feature selection method depends on the data and the problem at hand. Filter methods are fast and computationally efficient, but they may not capture complex interactions between features. Wrapper methods are more computationally expensive but can capture complex interactions between features and are suitable for models that do not have built-in feature selection capabilities. Embedded methods are efficient and suitable for models that have built-in feature selection capabilities.\n",
    "\n",
    "In summary, feature selection is a crucial step in machine learning, and it involves selecting the most relevant features to improve the model's accuracy and generalization ability. Various methods can be used for feature selection, including filter methods, wrapper methods, and embedded methods, and the choice of method depends on the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c98b05",
   "metadata": {},
   "source": [
    "### Q3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?\n",
    "\n",
    "ANSWER : Filter and wrapper approaches are two common methods for feature selection in machine learning.\n",
    "\n",
    "__`Filter Approach` :__ The filter approach selects features based on some statistical measures, such as correlation, mutual information, or statistical tests. The features are ranked according to their relevance to the target variable and a fixed number of top-ranked features are selected.\n",
    "\n",
    "Pros of Filter Approach:\n",
    "\n",
    "1. It is computationally efficient and can handle large datasets.\n",
    "2. It is less prone to overfitting.\n",
    "3. It is easy to implement and requires less computation.\n",
    "\n",
    "\n",
    "Cons of Filter Approach:\n",
    "\n",
    "1. It ignores the interaction between features.\n",
    "2. It may select redundant or irrelevant features.\n",
    "\n",
    "\n",
    "__`Wrapper Approach` :__ The wrapper approach selects features by using a machine learning model to evaluate the performance of different feature subsets. It trains the model with different feature subsets, evaluates their performance, and selects the subset that performs the best.\n",
    "\n",
    "Pros of Wrapper Approach:\n",
    "\n",
    "1. It can capture the interaction between features.\n",
    "2. It can lead to better performance in complex problems.\n",
    "3. It can reduce the risk of selecting irrelevant features.\n",
    "\n",
    "\n",
    "Cons of Wrapper Approach:\n",
    "\n",
    "1. It is computationally expensive and time-consuming.\n",
    "2. It is prone to overfitting when the dataset is small.\n",
    "3. It requires selecting the appropriate machine learning model to evaluate the feature subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1314f55",
   "metadata": {},
   "source": [
    "### Q4. i. Describe the overall feature selection process.\n",
    "\n",
    "ANSWER : (i)\n",
    "The overall feature selection process is a series of steps to identify and select a subset of relevant features from a larger set of features in a given dataset. The purpose of feature selection is to improve the performance of a machine learning model by reducing the dimensionality of the input data, reducing the risk of overfitting, and improving the model's interpretability.\n",
    "\n",
    "The overall feature selection process typically involves the following steps:\n",
    "\n",
    "1. Define the problem: Clearly define the problem and the goals of the feature selection process.\n",
    "\n",
    "2. Data pre-processing: Clean the data, remove missing values, and handle outliers.\n",
    "\n",
    "3. Feature selection: Select a subset of relevant features using one of the feature selection methods, such as filter or wrapper approaches.\n",
    "\n",
    "4. Model training: Train a machine learning model using the selected features.\n",
    "\n",
    "5. Model evaluation: Evaluate the performance of the model using a test dataset and compare it with the performance of a model trained on all features.\n",
    "\n",
    "6. Iteration: If the performance is not satisfactory, repeat the feature selection process using a different method or parameters until the desired performance is achieved.\n",
    "\n",
    "\n",
    "### ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n",
    "ANSWER(ii) : The key underlying principle of feature extraction is to identify and extract the underlying structure and patterns in the data that are relevant to the problem at hand.\n",
    "\n",
    "`Example of feature extraction` is in computer vision tasks such as image recognition. In this case, the raw input data is an image, and feature extraction techniques can be used to extract relevant features such as edges, corners, and shapes from the image. These extracted features can then be used as input to machine learning algorithms for tasks such as object recognition, face recognition, or image segmentation.\n",
    "\n",
    "The most widely used feature extraction algorithms include:\n",
    "\n",
    "1. Principal Component Analysis (PCA): A linear transformation technique that identifies the most important features that explain the maximum variance in the data.\n",
    "\n",
    "2. Independent Component Analysis (ICA): A technique that separates the input data into independent components that are statistically independent of each other.\n",
    "\n",
    "3. Linear Discriminant Analysis (LDA): A technique that maximizes the separation between classes by projecting the input data onto a lower-dimensional subspace.\n",
    "\n",
    "4. Wavelet Transform: A technique that decomposes the input data into different frequency bands to capture both the time and frequency information in the data.\n",
    "\n",
    "5. Convolutional Neural Networks (CNNs): A deep learning technique that automatically learns relevant features from the input data by training a neural network on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428aaf1a",
   "metadata": {},
   "source": [
    "### Q5. Describe the feature engineering process in the sense of a text categorization issue.\n",
    "\n",
    "ANSWER : Feature engineering is a crucial step in the text categorization process that involves extracting relevant features from raw text data to facilitate machine learning algorithms. Here, the goal is to classify text documents into pre-defined categories, such as positive or negative sentiment, or topic categories like sports, politics, or entertainment etc. The feature engineering process involves the following steps:\n",
    "\n",
    "1. `Text preprocessing` : The first step in feature engineering is to _preprocess the raw text data_ by removing stop words, punctuations, and other noise. The text is then tokenized into individual words, and any _stemming_ or _lemmatization_ is applied to reduce words to their base form.\n",
    "\n",
    "\n",
    "2. `Feature extraction` : The next step is to extract features from the preprocessed text data. There are various feature extraction techniques that can be used for text data, such as _Bag-of-Words, n-grams,_ and _Word embeddings._ \n",
    "    * In the `Bag-of-Words` model, each document is represented as a vector of word counts, where each element corresponds to the frequency of a specific word in the document. \n",
    "    * `N-grams` captures the co-occurrence of words within a specific window size. \n",
    "    * `Word embeddings` are dense vector representations of words that capture their semantic and syntactic meaning.\n",
    "\n",
    "\n",
    "3. `Feature selection` : Once the features are extracted, feature selection techniques can be used to reduce the dimensionality of the feature space and remove irrelevant or redundant features. This can improve the performance of the machine learning algorithm and reduce the risk of overfitting.\n",
    "\n",
    "\n",
    "4. `Model training` : The final step is to train a machine learning algorithm using the selected features. Common machine learning algorithms for text categorization include Naive Bayes, Support Vector Machines, and Deep Learning models like Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee766fd",
   "metadata": {},
   "source": [
    "### Q6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine.\n",
    "\n",
    "ANSWER : Cosine similarity is a popular metric for text categorization because it measures the similarity between two documents based on the angle between their feature vectors. In the context of text data, the feature vectors typically represent the word frequency counts or TF-IDF scores of each document. Cosine similarity ranges from -1 to 1, with 1 indicating that the two documents are identical and 0 indicating that they are completely dissimilar.\n",
    "\n",
    "The advantage of cosine similarity for text categorization is that it is robust to differences in document length and word frequency. It only considers the relative frequency of words in the two documents, rather than their absolute frequency. This makes it particularly useful for tasks such as document retrieval or search engines, where the length and content of the query document may differ from the target documents.\n",
    "\n",
    "Lets assume, \n",
    "x = {2, 3, 2, 0, 2, 3, 3, 0, 1}, \n",
    "y = {2, 1, 0, 0, 3, 2, 1, 3, 1}\n",
    "\n",
    "To calculate the cosine similarity between the two rows of the document-term matrix, we first need to compute the dot product of the two vectors:\n",
    "\n",
    "    x.y = (2 * 2) + (3 * 1) + (2 * 0) + (0 * 0) + (2 * 3) + (3 * 2) + (3 * 1) + (0 * 3) + (1 * 1) = 23\n",
    "\n",
    "Next, we need to compute the magnitude of each vector:\n",
    "\n",
    "    ||x|| = sqrt(2^2 + 3^2 + 2^2 + 0^2 + 2^2 + 3^2 + 3^2 + 0^2 + 1^2) = 6.32\n",
    "\n",
    "    ||y|| = sqrt(2^2 + 1^2 + 0^2 + 0^2 + 3^2 + 2^2 + 1^2 + 3^2 + 1^2) = 5.38\n",
    "\n",
    "Finally, we can calculate the cosine similarity as the dot product divided by the product of the magnitudes:\n",
    "\n",
    "    cosine_similarity = 23 / (6.32 * 5.38) = 0.676\n",
    "\n",
    "Therefore, the resemblance in cosine between the two rows is approximately 0.676."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53c3ca9",
   "metadata": {},
   "source": [
    "### Q7. (i) What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "\n",
    "ANSWER : The Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other.\n",
    "\n",
    "The formula for calculating Hamming distance is:\n",
    "        Hamming distance = number of positions where the two binary strings differ\n",
    "\n",
    "For example, the Hamming distance between `10001011` and `11001111` is 2 because the second and sixth bits differ. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec8f1a",
   "metadata": {},
   "source": [
    "### Q7. (ii) Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    "\n",
    "ANSWER : __`Jaccard Index` :__ Given two objects, A and B, each with `n` binary attributes, the `Jaccard coefficient` is a useful measure of the overlap that A and B share with their attributes. Each attribute of A and B can either be 0 or 1. The total number of each combination of attributes for both A and B are specified as follows:\n",
    "\n",
    "A = (1, 1, 0, 0, 1, 0, 1, 1)\n",
    "\n",
    "B = (1, 1, 0, 0, 0, 1, 1, 1)\n",
    "\n",
    "* M11 : total number of attributes where A and B both have a value of 1.\n",
    "* M01 : total number of attributes where the attribute of A is 0 and the attribute of B is 1.\n",
    "* M10 : total number of attributes where the attribute of A is 1 and the attribute of B is 0.\n",
    "* M00 : total number of attributes where A and B both have a value of 0.\n",
    "\n",
    "Each attribute must fall into one of these four categories, meaning that, n = M00 + M01 + M10 + M11\n",
    "\n",
    "Jaccard(A, B) = M11 / (M01 + M10 + M11) = 4 / (1 + 1 + 4) = 0.667\n",
    "\n",
    "__`Simple Matching Coefficient (SMC)`__ or Rand similarity coefficient is a statistic used for comparing the similarity and diversity of sample sets.\n",
    "\n",
    "SMC = (M00 + M11) / (M00 + M01 + M10 + M11) = sum of matching attributes / Sum of all attributes\n",
    "\n",
    "SMC = (2 + 4) / (2 + 1 + 1 + 4) = 0.75\n",
    "\n",
    "COMPARISON : When used for binary attributes, the Jaccard index is very similar to the simple matching coefficient. The main difference is that the SMC has the term `M00` in its numerator and denominator, whereas the Jaccard index does not. Thus, the SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts mutual presence as matches and compares it to the number of attributes that have been chosen by at least one of the two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051dc6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "y1 = np.array([1, 1, 0, 0, 1, 0, 1, 1])\n",
    "y2 = np.array([1, 1, 0, 0, 0, 1, 1, 1])\n",
    "jaccard_score(y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98553651",
   "metadata": {},
   "source": [
    "### Q8. State what is meant by \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?\n",
    "\n",
    "ANSWER : \n",
    "* High-dimensional data sets are those that have a large number of features or dimensions relative to the number of observations or data points. In other words, the data is represented by many variables or attributes, which can make it difficult to analyze and interpret.\n",
    "\n",
    "\n",
    "* Real-life examples of high-dimensional data sets include:\n",
    "    1. Genomic data : this may include hundreds or thousands of genes that are being analyzed across a large number of samples\n",
    "    2. Images or videos : this can contain millions of pixels or frames that are being analyzed for various features or patterns\n",
    "    3. Sensor data from Internet of Things (IoT) devices, which may include data from multiple sensors or sources that are being collected at a high frequency.\n",
    "\n",
    "\n",
    "* The main difficulty in using machine learning techniques on high-dimensional data sets is the \"curse of dimensionality\". This refers to the fact that as the number of dimensions increases, the amount of data required to make accurate predictions or classifications also increases exponentially. Additionally, high-dimensional data sets can be prone to overfitting, where the model fits the noise in the data rather than the underlying patterns.\n",
    "\n",
    "\n",
    "* To address these issues, various techniques can be used, including:\n",
    "\n",
    "    1. Feature selection, which involves selecting a subset of the most important features to reduce the dimensionality of the data set\n",
    "    2. Dimensionality reduction, which involves transforming the data into a lower-dimensional space while preserving the most important information\n",
    "    3. Regularization, which involves adding a penalty term to the model to prevent overfitting\n",
    "    4. Ensemble methods, which involve combining multiple models to improve performance and reduce overfitting.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c51b04",
   "metadata": {},
   "source": [
    "### Q9. Make a few quick notes on:\n",
    "\n",
    "### 1. PCA is an acronym for Personal Computer Analysis. \n",
    "\n",
    "This statement is incorrect. PCA is an acronym for Principal Component Analysis, which is a widely used technique in statistics and machine learning for reducing the dimensionality of high-dimensional data sets. PCA involves transforming the data into a lower-dimensional space while preserving the most important information. This can make it easier to analyze and visualize the data, and can also help to reduce overfitting and improve the performance of machine learning models.\n",
    "\n",
    "### 2. Use of vectors\n",
    "\n",
    "Vectors are commonly used in machine learning as a way to represent data and model relationships between variables and are typically a one-dimensional array or list of numbers that represents a single data point or observation. Vectors can be used to represent a wide range of data types, including text, images, audio, and sensor readings.\n",
    "\n",
    "ADVANTAGE : Vectors can be easily manipulated and analyzed using mathematical operations such as dot products, matrix multiplications, and vector norms. This allows machine learning models to learn patterns and relationships between variables, and make predictions based on these relationships.\n",
    "\n",
    "In addition, vectors can be used to represent feature spaces, which are the set of all possible values that a variable can take. By representing variables as vectors in a feature space, machine learning models can identify patterns and correlations between variables, and use this information to make accurate predictions.\n",
    "\n",
    "### 3. Embedded technique\n",
    "\n",
    "Embedded technique is a type of feature selection method in machine learning, where feature selection is performed as part of the model training process. In other words, the selection of features is \"embedded\" within the model building process, rather than being performed as a separate preprocessing step.\n",
    "\n",
    "Embedded techniques work by incorporating a penalty term into the model optimization process that encourages the model to select the most important features while penalizing the inclusion of irrelevant or redundant features. This penalty term is typically based on some form of regularization, such as L1 or L2 regularization.\n",
    "\n",
    "Advantage : Embedded technique can lead to more efficient and accurate models by automatically selecting the most relevant features for the specific problem at hand. Additionally, by incorporating feature selection into the model building process, embedded techniques can help to reduce overfitting and improve the generalization performance of the model.\n",
    "\n",
    "Examples : LASSO regression, Ridge regression, and Elastic Net regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a597497",
   "metadata": {},
   "source": [
    "### Q10. Make a comparison between:\n",
    "\n",
    "### 1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "Sequential backward exclusion:\n",
    "\n",
    "1. The method starts with all features included in the model and iteratively removes one feature at a time based on its impact on the model performance.\n",
    "2. At each iteration, the feature with the smallest impact on the model performance is removed until the desired number of features is reached.\n",
    "3. This method is generally more computationally efficient than sequential forward selection since it only requires fitting the model once for the initial set of features and then iteratively removing features.\n",
    "\n",
    "Sequential forward selection:\n",
    "\n",
    "1. The method starts with an empty feature set and iteratively adds one feature at a time based on its impact on the model performance.\n",
    "2. At each iteration, the feature that provides the largest improvement in the model performance is added until the desired number of features is reached.\n",
    "3. This method may require fitting the model multiple times, which can be computationally expensive, but it may identify more complex feature combinations than sequential backward exclusion.\n",
    "\n",
    "\n",
    "### 2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "Filter methods involve selecting features based on their statistical properties, such as their correlation with the target variable or their mutual information with other features. Filter methods are generally fast and computationally efficient, and can be applied to large datasets. However, filter methods do not consider the interactions between features and may not identify the best subset of features for a particular problem.\n",
    "\n",
    "Wrapper methods, on the other hand, involve selecting features based on how well they improve the performance of a particular machine learning model. Wrapper methods use a specific model to evaluate different subsets of features, and select the subset that results in the best performance. Wrapper methods can identify the best subset of features for a specific problem and can capture interactions between features. However, wrapper methods can be computationally expensive, as they require training and evaluating multiple models.\n",
    "\n",
    "\n",
    "### 3. SMC vs. Jaccard coefficient\n",
    "\n",
    "The SMC (Simple Matching Coefficient) and Jaccard coefficient are both similarity measures commonly used in machine learning and data analysis for comparing binary data sets. Here is a comparison between the two:\n",
    "\n",
    "1. `Definition` : The SMC measures the proportion of matching elements between two sets, while the Jaccard coefficient measures the proportion of elements that are common to both sets relative to the total number of unique elements in the sets.\n",
    "\n",
    "2. `Calculation` : The SMC is calculated as the number of matching elements divided by the total number of elements in the sets, while the Jaccard coefficient is calculated as the number of common elements divided by the total number of unique elements in the sets.\n",
    "\n",
    "3. `Range` : The SMC ranges from 0 to 1, with a higher value indicating a greater degree of similarity, while the Jaccard coefficient also ranges from 0 to 1 but can be more sensitive to small differences in the data.\n",
    "\n",
    "4. `Use case` : The SMC is commonly used when the size of the data sets being compared is relatively small, while the Jaccard coefficient is more suitable for larger data sets where there may be a high degree of sparsity or imbalance between the sets.\n",
    "\n",
    "5. `Interpretation` : The SMC is often interpreted as a measure of overall similarity between two sets, while the Jaccard coefficient is often interpreted as a measure of similarity between the overlapping portions of the sets.\n",
    "\n",
    "In summary, while both SMC and Jaccard coefficient are useful for comparing binary data sets, they have different strengths and weaknesses that make them more suitable for different use cases. The SMC is more appropriate for smaller data sets and provides an overall similarity measure, while the Jaccard coefficient is more suitable for larger and sparse data sets and provides a measure of similarity between overlapping portions of the sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
