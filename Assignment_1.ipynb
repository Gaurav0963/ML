{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e99639",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee741852",
   "metadata": {},
   "source": [
    "### Q1. What does one mean by the term \"machine learning\"?\n",
    "\n",
    "Answer :\n",
    "\n",
    "`Machine learning` is a subfield of artificial intelligence (AI) that involves using algorithms and statistical models to enable computers to learn from data, identify patterns, and make predictions or decisions without being explicitly programmed to do so.\n",
    "\n",
    "Machine Learning can also be defined as a method of training computers to learn from examples, recognize patterns, and make decisions based on data. This approach allows computers to improve their performance on a task over time as they receive more data and feedback. Machine learning algorithms are used in a variety of applications, including image recognition, speech recognition, natural language processing, recommendation systems, and predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb961b",
   "metadata": {},
   "source": [
    "### Q2. Can you think of 4 distinct types of issues where it shines?\n",
    "\n",
    "Answer : \n",
    "\n",
    "1. `Image and Object Recognition` : Machine learning is particularly effective at *recognizing patterns* in images and *identifying objects* within them. This has led to applications such as facial recognition, self-driving cars, and image-based search engines.\n",
    "\n",
    "\n",
    "2. `Natural Language Processing` : Machine learning algorithms can also be used to analyze and understand human language, which has a wide range of applications such as chatbots, virtual assistants, and sentiment analysis of social media data.\n",
    "\n",
    "\n",
    "3. `Fraud Detection` : Machine learning is very effective at identifying patterns of fraudulent behavior and anomalies in data that may indicate fraudulent activity. This can be used in a variety of industries, such as finance, healthcare, and e-commerce.\n",
    "\n",
    "\n",
    "4. `Predictive Analytics` : Machine learning can be used to build predictive models that can forecast future events or behavior based on historical data. This can be useful in industries such as marketing, healthcare, and finance, where accurate predictions can help businesses make more informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74584ba8",
   "metadata": {},
   "source": [
    "### Q3. What is a labeled training set, and how does it work?\n",
    "\n",
    "Answer : A labeled training set is a collection of data used to train a machine learning model. Each data point in the set consists of an input (or feature) and an output (or __label__). The model learns to predict the output based on the input by analyzing the patterns in the training data. Once trained, the model can then be used to make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cba5f",
   "metadata": {},
   "source": [
    "### Q4. What are the two most important tasks that are supervised?\n",
    "\n",
    "Answer : Two most important tasks that are supervised are enumerated below:\n",
    "\n",
    "1. __`Regression`__ : Regression is a supervised learning task that involves predicting a continuous output variable based on input features. In regression, the goal is to build a model that can accurately predict a numerical value, such as the price of a house or the temperature of a city, based on a set of input features. The output variable is continuous, meaning that it can take on any value within a range.\n",
    "\n",
    "\n",
    "2. __`Classification`__ : Classification is a supervised learning task that involves predicting a categorical output variable based on input features. In classification, the goal is to build a model that can accurately predict a label or category, such as whether an email is spam or not, based on a set of input features. The output variable is categorical, meaning that it can take on a limited number of discrete values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6df7ed",
   "metadata": {},
   "source": [
    "### Q5. Can you think of four examples of unsupervised tasks?\n",
    "\n",
    "Answer : Few of the examples of unsupervised tasks are enumerated below.\n",
    "\n",
    "1. `Clustering` : Clustering is an unsupervised learning task that involves grouping similar data points together based on their characteristics or features. This can be used to identify patterns in the data, segment customers into different groups, or identify anomalies or outliers.\n",
    "\n",
    "\n",
    "2. `Dimensionality Reduction` : Dimensionality reduction is an unsupervised learning task that involves reducing the number of features in a dataset while preserving as much of the relevant information as possible. This can be used to visualize high-dimensional data, speed up training times, or remove noise from the data.\n",
    "\n",
    "\n",
    "3. `Anomaly Detection` : Anomaly detection is an unsupervised learning task that involves identifying data points that are significantly different from the majority of the data. This can be used to identify fraudulent transactions, diagnose medical conditions, or detect faults in machinery.\n",
    "\n",
    "\n",
    "4. `Association Rule Learning` : Association rule learning is an unsupervised learning task that involves identifying patterns or relationships between different variables in a dataset. This can be used to identify which products are frequently bought together in a retail store, or which symptoms are often associated with a particular medical condition.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04178e72",
   "metadata": {},
   "source": [
    "### Q6. State the machine learning model that would be best to make a robot walk through various unfamiliar terrains?\n",
    "\n",
    "Answer : A `reinforcement learning` model is a good choice for making a robot walk through various unfamiliar terrains as it allows the robot to learn and adapt to new environments without relying on pre-defined rules or patterns. The reinforcement learning model would learn through trial and error, receiving feedback in the form of rewards or penalties for its actions as it navigates the terrain. Over time, the model would learn to make decisions that result in the greatest rewards, such as successfully traversing the terrain without falling or getting stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc23e0bf",
   "metadata": {},
   "source": [
    "### Q7. Which algorithm will you use to divide your customers into different groups?\n",
    "\n",
    "Answer : __Clustering Algorithm__ would be the goto approach used for dividing customers into different groups. Clustering algorithms group data points (in this case, customers) based on their similarity. There are many different clustering algorithms to choose from, and the best one for your specific use case will depend on the characteristics of your data and the goals of your analysis. Some popular clustering algorithms include k-means, hierarchical clustering, and DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c831c",
   "metadata": {},
   "source": [
    "### Q8. Will you consider the problem of spam detection to be a supervised or unsupervised learning problem?\n",
    "\n",
    "Answer : The `spam detection` problem is typically considered a __supervised learning__ problem. In supervised learning, the algorithm is trained on a labeled dataset where the correct output (in this case, whether an email is spam or not) is provided for each input (the contents of the email)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0026975c",
   "metadata": {},
   "source": [
    "### Q9. What is the concept of an online learning system?\n",
    "\n",
    "Answer : An online learning system is a machine learning system that continuously learns and updates its model in real-time as new data becomes available. In contrast to batch learning, where the system is trained on a fixed dataset and then deployed to make predictions on new data, online learning allows the system to learn and adapt to new data as it arrives.\n",
    "\n",
    "In an online learning system, data is typically processed in small batches or one data point at a time, and the model is updated incrementally with each new batch or data point. This allows the system to quickly adapt to changes in the data distribution or to identify new patterns or trends in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d7562",
   "metadata": {},
   "source": [
    "### Q10. What is out-of-core learning, and how does it differ from core learning?\n",
    "\n",
    "Answer : Out-of-core algorithms can handle vast quantities of data that cannot fit in a computer’s main memory. An out-of-core learning algorithm chops the data into mini-batches and uses online learning techniques to learn from these mini-batches.\n",
    "Whereas, in-core learning, also known as batch learning, involves loading the entire dataset into memory and training the model on the entire dataset at once.\n",
    "\n",
    "The main difference between out-of-core learning and in-core learning is that out-of-core learning is designed to handle datasets that are too large to fit into memory, whereas in-core learning assumes that the entire dataset can be loaded into memory at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfc02eb",
   "metadata": {},
   "source": [
    "### Q11. What kind of learning algorithm makes predictions using a similarity measure?\n",
    "\n",
    "Answer : \n",
    "A learning algorithm that makes predictions using a similarity measure is a type of instance-based learning algorithm called k-nearest neighbors (k-NN). `Instance-based learning` : The system learns the examples by heart, then generalises to new cases Ey using a similarity measure to compare them to the learned examples (or a subset of them).\n",
    "\n",
    "In k-NN, the algorithm learns by storing the entire training dataset, and making predictions for new data points by comparing them to the training examples that are closest to them in the feature space. The number of nearest neighbors considered is specified by the hyperparameter k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa4ae6",
   "metadata": {},
   "source": [
    "### Q12. What's the difference between a model parameter and a hyperparameter in a learning algorithm?\n",
    "\n",
    "Answer : \n",
    "\n",
    "__Model Parameter :__ A model parameter is a variable that is learned during training. Model parameters are variables that the learning algorithm tries to optimize during training to minimize the error between the predicted output and the actual output. These parameters are specific to the learning algorithm and the model architecture and are learned from the training data.\n",
    "\n",
    "Examples : weights and biases in a neural network, coefficients in a linear regression model, or the splitting rules in a decision tree.\n",
    "\n",
    "__Hyperparameter :__ A hyperparameter is a setting that is set before training and controls the behavior of the learning algorithm. Hyperparameters are set by the user or selected by a tuning algorithm before training and control the behavior of the learning algorithm. These parameters are not learned from the data, but rather are set based on the characteristics of the data and the desired behavior of the model. \n",
    "\n",
    "Examples : learning rate in gradient descent optimization, the number of hidden layers in a neural network, or the regularization parameter in a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814300d",
   "metadata": {},
   "source": [
    "### Q13. What are the criteria that model-based learning algorithms look for? What is the most popular method they use to achieve success? What method do they use to make predictions?\n",
    "\n",
    "Answer : Model-based learning algorithms aim to learn a mathematical representation or model of the relationship between the input features and the output variable. These algorithms typically look for a model that can accurately predict the output variable for new input examples.\n",
    "\n",
    "The criteria that model-based learning algorithms look for can vary depending on the specific algorithm and the task at hand. However, some common criteria include:\n",
    "\n",
    "1. `Accuracy` : The model should be able to accurately predict the output variable for new input examples.\n",
    "\n",
    "2. `Complexity` : The model should not be too complex, as overly complex models can overfit to the training data and perform poorly on new data.\n",
    "\n",
    "3. `Interpretabilty` : In some cases, it may be desirable for the model to be interpretable, meaning that it is easy to understand how the model is making predictions.\n",
    "\n",
    "The most popular method that model-based learning algorithms use to achieve success is to __optimize a loss function__ during training. The loss function measures the error between the predicted output and the actual output, and the learning algorithm adjusts the model parameters to minimize this error. Popular optimization algorithms include gradient descent and its variants.\n",
    "\n",
    "To make predictions, model-based learning algorithms apply the learned model to new input examples. This involves feeding the input features into the model, which then produces a predicted output value based on the learned relationships between the input features and the output variable. The specific method used to make predictions can vary depending on the model architecture and the task at hand. For example, a linear regression model makes predictions based on a weighted sum of the input features, while a decision tree model makes predictions based on a series of binary decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90deaf1d",
   "metadata": {},
   "source": [
    "### Q14. Can you name four of the most important Machine Learning challenges?\n",
    "\n",
    "Answer : \n",
    "1. `Data quality and quantity` : One of the biggest challenges in machine learning is obtaining enough high-quality data to train the algorithms effectively. Without enough data, the models may not be accurate enough, and poor quality data can lead to biased or unreliable results.\n",
    "\n",
    "\n",
    "2. `Model complexity` : Another challenge is finding the right balance between model complexity and simplicity. Models that are too simple may not capture the nuances of the data, while models that are too complex may overfit to the training data and not generalize well to new data.\n",
    "\n",
    "\n",
    "3. `Interpretability` : Many machine learning models are black boxes, meaning that it can be difficult to understand how they are making predictions. In some cases, interpretability is important for ethical, legal, or practical reasons, and so developing models that are more transparent and interpretable is an ongoing challenge.\n",
    "\n",
    "\n",
    "4. `Scalability` : As the amount of data being generated continues to grow exponentially, the ability to scale machine learning algorithms to handle large datasets is becoming increasingly important. Scaling up algorithms to handle large volumes of data is a complex challenge that requires careful consideration of hardware, software, and algorithmic optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e468876f",
   "metadata": {},
   "source": [
    "### Q15. What happens if the model performs well on the training data but fails to generalize the results to new situations? Can you think of three different options?\n",
    "\n",
    "Answer : If a model performs well on the training data but fails to generalize to new situations, it is likely __overfitting__ to the __training data__. Here are three different options to address this issue:\n",
    "\n",
    "1. __Use a larger and more diverse dataset:__ One way to address overfitting is to increase the amount and diversity of the data used to train the model. This can help the model to learn more general patterns in the data, rather than simply memorizing the training examples.\n",
    "\n",
    "2. __Simplify the model:__ Another option is to reduce the complexity of the model. This can be done by removing features, decreasing the number of hidden layers in a neural network, or reducing the number of decision trees in a random forest. By simplifying the model, it becomes less likely to overfit to the training data.\n",
    "\n",
    "3. __Use regularization techniques:__ Regularization is a set of techniques used to prevent overfitting by adding additional constraints to the model during training. Examples of regularization techniques include L1 and L2 regularization, which add penalties to the model's loss function for large weights, and dropout, which randomly drops out some neurons during training to prevent over-reliance on any one feature.\n",
    "\n",
    "NOTE : the best approach to addressing overfitting will depend on the specific problem, the available data, and the type of model being used. It's often necessary to try several different approaches to find the best solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea20c4",
   "metadata": {},
   "source": [
    "### Q16. What exactly is a test set, and why would you need one?\n",
    "\n",
    "Answer : In machine learning, a test set is a set of data that is used to evaluate the performance of a trained model. The test set is separate from the training set and is not used during the training phase. Instead, the test set is used to simulate real-world scenarios by feeding new, unseen data to the model and measuring how accurately the model makes predictions.\n",
    "\n",
    "The need for a test set arises because, in machine learning, the goal is to create a model that can accurately predict outcomes for new, unseen data. Therefore, it's important to evaluate the model's performance on data that it has not been trained on to ensure that it can generalize well to new situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba8840",
   "metadata": {},
   "source": [
    "### Q17. What is a validation set's purpose?\n",
    "\n",
    "Answer : The purpose of a validation set in machine learning is to : \n",
    "1. evaluate the __performance__ of a model during the training phase and help to prevent overfitting.\n",
    "\n",
    "2. to tune the hyperparameters of the model, such as the learning rate, regularization parameters, or the number of hidden layers in a neural network. By evaluating the model's performance on the validation set, you can choose the best hyperparameters for the model.\n",
    "\n",
    "In summary, a validation set is used to evaluate the performance of a model during the training phase, prevent overfitting, and tune the hyperparameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e7f71",
   "metadata": {},
   "source": [
    "### Q18. What precisely is the train-dev kit, when will you need it, how do you put it to use?\n",
    "\n",
    "Answer : The train-dev set, also known as the development set, is a subset of the training data that is used when there is a risk of mismatch between the training data and the data used in the validation and test datasets (which should always be as close as possible to the data used once the model is in production). The train-dev set is a part of the training set that’s held out (the model is not trained on it). The model is trained on the rest of the training set, and evaluated on both the train-dev set and the validation set. \n",
    "* If the model performs well on the training set but not on the train-dev set, then the model is likely overfitting the training set. \n",
    "* If it performs well on both the training set and the train-dev set, but not on the validation set, then there is probably a significant data mismatch between the training data and the validation + test data, and you should try to improve the training data to make it look more like the validation + test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e7c8b",
   "metadata": {},
   "source": [
    "### Q19. What could go wrong if you use the test set to tune hyperparameters?\n",
    "\n",
    "Answer : The purpose of the test set is to evaluate the performance of the final model after it has been fully trained and optimized on the training data. It should not be used to make decisions about the model architecture, hyperparameters, or other aspects of the model development process because if you use the test set to tune hyperparameters, you risk _overfitting to the test set, which can lead to over-optimistic performance estimates and poor generalization to new, unseen data._ These problems will occur because the test set is no longer an independent evaluation dataset, but rather a part of the training process. This can lead to overfitting to the test set, where the model is optimized to perform well on the test set but does not generalize well to new data.\n",
    "\n",
    "To avoid this issue, you should use a separate validation set to tune hyperparameters during the model development process. The validation set should be separate from both the training set and the test set and used solely for hyperparameter tuning and model selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
